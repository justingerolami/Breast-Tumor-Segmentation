{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "The data used in this notebook is not public. \\\n",
    "This code is partially adapted from and was originally part of the SlicerIGT/AIGT repository (https://github.com/SlicerIGT/aigt). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from random import sample\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import girder_client\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import sklearn\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import ResUNet_Model as resunet\n",
    "# import ultrasound_batch_generator as generator\n",
    "# import sagittal_spine_segmentation_unet as unet\n",
    "import evaluation_metrics\n",
    "\n",
    "from girder_apikey_read import girder_apikey_read\n",
    "girder_api_url = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "this_notebook_name = \"ResUNet-BreastTumorSegmentationStudyTest\"\n",
    "\n",
    "# Update this folder name for your computer\n",
    "train_timestamp = \"2020-01-27_17-20-14\"\n",
    "local_data_folder = r\"/home/justin/Desktop/BreastTumorSegmentationStudy\"\n",
    "overwrite_existing_data_files = False\n",
    "\n",
    "# Evaluation parameters\n",
    "acceptable_margin_mm = 1.0\n",
    "mm_per_pixel = 1.0\n",
    "acceptable_margin_pixel = int(acceptable_margin_mm/mm_per_pixel)\n",
    "roc_thresholds = [0.9, 0.8, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1,\n",
    "                  0.08, 0.06, 0.04, 0.02, 0.01,\n",
    "                  0.008, 0.006, 0.004, 0.002, 0.001]\n",
    "\n",
    "limit_rounds = 0\n",
    "\n",
    "# Uncomment for faster debugging\n",
    "roc_thresholds = [0.8, 0.6, 0.4, 0.2, 0.1, 0.01]\n",
    "limit_rounds = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_ultrasound_ids = [\n",
    "    \"5e1cb685d9e6a3be02d01434\", #006-v02\n",
    "    \"5e1cbf54d9e6a3be02d0144c\", #012\n",
    "    \"5e1cca85d9e6a3be02d01464\", #018\n",
    "    \"5e163f22d9e6a3be02d01398\", #024\n",
    "    #\"5e1c99f3d9e6a3be02d0140d\", #030\n",
    "]\n",
    "\n",
    "testing_ultrasound_filenames = [\n",
    "    \"ultrasound-006-v02.npy\",\n",
    "    \"ultrasound-012.npy\",\n",
    "    \"ultrasound-018.npy\",\n",
    "    \"ultrasound-024.npy\",\n",
    "    #\"ultrasound-030.npy\",\n",
    "]\n",
    "\n",
    "testing_segmentation_ids = [\n",
    "    \"5e1cb684d9e6a3be02d01431\", #006-v02\n",
    "    \"5e1cbf54d9e6a3be02d01449\", #012\n",
    "    \"5e1cca84d9e6a3be02d01461\", #018\n",
    "    \"5e163f21d9e6a3be02d01395\", #024\n",
    "    #\"5e1c99eed9e6a3be02d013ec\", #030\n",
    "]\n",
    "\n",
    "testing_segmentation_filenames = [\n",
    "    \"segmentation-006-v02.npy\",\n",
    "    \"segmentation-012.npy\",\n",
    "    \"segmentation-018.npy\",\n",
    "    \"segmentation-024.npy\",\n",
    "    #\"segmentation-030.npy\",\n",
    "]\n",
    "\n",
    "# Default subfolders of main project data folder\n",
    "data_arrays_folder      = \"DataArrays\"\n",
    "notebooks_save_folder   = \"SavedNotebooks\"\n",
    "models_folder           = \"SavedModels\"\n",
    "results_save_folder     = \"SavedResults\"\n",
    "test_predictions_folder = \"PredictionsTest\"\n",
    "\n",
    "data_arrays_fullpath      = os.path.join(local_data_folder, data_arrays_folder)\n",
    "notebooks_save_fullpath   = os.path.join(local_data_folder, notebooks_save_folder)\n",
    "models_fullpath           = os.path.join(local_data_folder, models_folder)\n",
    "results_save_fullpath     = os.path.join(local_data_folder, results_save_folder)\n",
    "test_predictions_fullpath = os.path.join(local_data_folder, test_predictions_folder)\n",
    "\n",
    "if not os.path.exists(data_arrays_fullpath):\n",
    "    os.makedirs(data_arrays_fullpath)\n",
    "    print(\"Created folder: {}\".format(data_arrays_fullpath))\n",
    "\n",
    "if not os.path.exists(notebooks_save_fullpath):\n",
    "    os.makedirs(notebooks_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(notebooks_save_fullpath))\n",
    "\n",
    "if not os.path.exists(models_fullpath):\n",
    "    raise FileNotFoundError(models_fullpath)\n",
    "\n",
    "if not os.path.exists(results_save_fullpath):\n",
    "    os.makedirs(results_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(results_save_fullpath))\n",
    "\n",
    "if not os.path.exists(test_predictions_fullpath):\n",
    "    os.makedirs(test_predictions_fullpath)\n",
    "    print(\"Created folder: {}\".format(test_predictions_fullpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Girder\n",
    "time_download_start = datetime.datetime.now()\n",
    "\n",
    "print(\"Downloading testing files ...\")\n",
    "\n",
    "# Preparing progress bar\n",
    "n_files = len(testing_ultrasound_ids)\n",
    "f = IntProgress(min=0, max=n_files*2)\n",
    "display(f)\n",
    "\n",
    "# Downloading files\n",
    "gclient = girder_client.GirderClient(apiUrl=girder_api_url)\n",
    "gclient.authenticate(apiKey=girder_apikey_read)\n",
    "\n",
    "for i in range(n_files):\n",
    "    testing_ultrasound_fullname = os.path.join(data_arrays_fullpath, testing_ultrasound_filenames[i])\n",
    "    if not os.path.exists(testing_ultrasound_fullname) or overwrite_existing_data_files:\n",
    "        print(\"Downloading {}...\".format(testing_ultrasound_fullname))\n",
    "        gclient.downloadFile(testing_ultrasound_ids[i], testing_ultrasound_fullname)\n",
    "    f.value = i * 2 + 1\n",
    "    \n",
    "    testing_segmentation_fullname = os.path.join(data_arrays_fullpath, testing_segmentation_filenames[i])\n",
    "    if not os.path.exists(testing_segmentation_fullname) or overwrite_existing_data_files:\n",
    "        print(\"Downloading {}...\".format(testing_segmentation_fullname))\n",
    "        gclient.downloadFile(testing_segmentation_ids[i], testing_segmentation_fullname)\n",
    "    f.value = i * 2 + 2\n",
    "\n",
    "time_download_stop = datetime.datetime.now()\n",
    "print(\"\\nTotal download time: {}\".format(time_download_stop - time_download_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into numpy arrays\n",
    "test_ultrasound_arrays = []\n",
    "test_segmentation_arrays = []\n",
    "\n",
    "f = IntProgress(min=0, max=n_files * 2)\n",
    "display(f)\n",
    "\n",
    "time_start = datetime.datetime.now()\n",
    "\n",
    "for i in range(n_files):\n",
    "    testing_ultrasound_fullname = os.path.join(data_arrays_fullpath, testing_ultrasound_filenames[i])\n",
    "    testing_segmentation_fullname = os.path.join(data_arrays_fullpath, testing_segmentation_filenames[i])\n",
    "\n",
    "    testing_ultrasound_data = np.load(testing_ultrasound_fullname)\n",
    "    f.value = i * 2 + 1\n",
    "    \n",
    "    testing_segmentation_data = np.load(testing_segmentation_fullname)\n",
    "    f.value = i * 2 + 2\n",
    "    \n",
    "    test_ultrasound_arrays.append(testing_ultrasound_data)\n",
    "    test_segmentation_arrays.append(testing_segmentation_data)\n",
    "\n",
    "time_stop = datetime.datetime.now()\n",
    "print(\"\\nTotal time to load from files: {}\".format(time_stop - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pattern = \"*\" + train_timestamp + \"*\"\n",
    "search_string = os.path.join(models_fullpath, filename_pattern)\n",
    "print(\"Searching for models by {}\".format(search_string))\n",
    "model_file_list = glob.glob(search_string)\n",
    "\n",
    "num_models = len(model_file_list)\n",
    "print(\"Found {} models\".format(num_models))\n",
    "\n",
    "if limit_rounds > 0:\n",
    "    num_rounds = min(num_models, limit_rounds)\n",
    "else:\n",
    "    num_rounds = num_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ultrasound_data = np.zeros(\n",
    "        [0, test_ultrasound_arrays[0].shape[1], test_ultrasound_arrays[0].shape[2], test_ultrasound_arrays[0].shape[3]])\n",
    "test_segmentation_data = np.zeros(\n",
    "        [0, test_ultrasound_arrays[0].shape[1], test_ultrasound_arrays[0].shape[2], test_ultrasound_arrays[0].shape[3]])\n",
    "\n",
    "for test_index in range(n_files):\n",
    "    test_ultrasound_data = np.concatenate((test_ultrasound_data, test_ultrasound_arrays[test_index]))\n",
    "    test_segmentation_data = np.concatenate((test_segmentation_data, test_segmentation_arrays[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of models: \", num_models)\n",
    "print(\"Testing ultrasound data shape:            {}\".format(test_ultrasound_data.shape))\n",
    "print(\"Using timestemp to find files: {}\".format(train_timestamp))\n",
    "print(\"Saving test predictions in:    {}\".format(test_predictions_fullpath))\n",
    "\n",
    "test_best_metrics    = dict()\n",
    "test_fuzzy_metrics   = dict()\n",
    "test_aurocs          = np.zeros(num_models)\n",
    "test_best_thresholds = np.zeros(num_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop, test all models and save test results\n",
    "\n",
    "#so we don't overflow the graph with nodes\n",
    "from tensorflow.keras.backend import clear_session\n",
    "time_sequence_start = datetime.datetime.now()\n",
    "\n",
    "threshold=0.5\n",
    "wcce = resunet.weighted_categorical_crossentropy([0.2, 0.8])\n",
    "\n",
    "totDistanceErrs = []\n",
    "totMissed = []\n",
    "totSeg = []\n",
    "img_FPs = []\n",
    "img_FNs = []\n",
    "img_TNs = []\n",
    "img_TPs = []\n",
    "img_ACCs = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    count=0\n",
    "    totDistanceErr = 0\n",
    "    img_FP = 0\n",
    "    img_FN = 0\n",
    "    img_TN = 0\n",
    "    img_TP = 0\n",
    "    missed = []\n",
    "    seg = []\n",
    "    \n",
    "    #Clear the session which contains graph nodes from the previous model. \n",
    "    #This speeds up testing and keeps each model uniform for testing time\n",
    "    clear_session()\n",
    "    time_round_start = datetime.datetime.now()\n",
    "    \n",
    "    print(\"Testing model: {}\".format(model_file_list[i]))\n",
    "    model = tf.keras.models.load_model(model_file_list[i], custom_objects={'loss': wcce, 'dice_coef': resunet.dice_coef})\n",
    "    test_prediction = model.predict(test_ultrasound_data)\n",
    "    \n",
    "    '''\n",
    "    Part 1 - remove predicted areas less than a threshold value\n",
    "    '''\n",
    "    test_prediction[:,:,:,0] = (test_prediction[:,:,:,0] > threshold)\n",
    "    test_prediction[:,:,:,1] = (test_prediction[:,:,:,1] > threshold)\n",
    "    #test_prediction[:,:,:,0] = 1-test_prediction[:,:,:,1]\n",
    "    \n",
    "    totImg = test_prediction.shape[0]\n",
    "    for k in range(totImg):\n",
    "\n",
    "        '''\n",
    "        Part 2 - Contour removal and center distance calculation\n",
    "        '''\n",
    "        #find all contours of prediction\n",
    "        pred_im2, pred_contours, pred_hierarchy = cv2.findContours(np.uint8(np.rint(test_prediction[k,:,:,1])),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        seg_im2, seg_contours, seg_hierarchy = cv2.findContours(np.uint8(np.rint(test_segmentation_data[k,:,:,0])),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        #some images may not have a contour predicted\n",
    "        if len(pred_contours) !=0 and len(seg_contours) !=0:   \n",
    "            #print(\"Length of contour array before removal:\", len(pred_contours))\n",
    "            #check if there are multiple\n",
    "            if len(pred_contours)>1:\n",
    "                contourAreas = []\n",
    "                #there are multiple, create a mask of the image and use it to remove smallest contours\n",
    "                mask = np.ones(test_prediction[k].shape[:2], dtype=\"uint8\")\n",
    "        \n",
    "                #loop through each contour, find area\n",
    "                for c in range(len(pred_contours)):\n",
    "                    M = cv2.moments(pred_contours[c])\n",
    "                    contourAreas.append(cv2.contourArea(pred_contours[c]))\n",
    "        \n",
    "                maxArea = max(contourAreas)\n",
    "                #print(\"Contour areas (largest should be kept):\", contourAreas)\n",
    "                #find position of largest contour\n",
    "                for c in range(len(contourAreas)):\n",
    "                    if contourAreas[c] < maxArea:\n",
    "                        #draw the smallest contours on the mask\n",
    "                        cv2.drawContours(mask, [pred_contours[c]], -1,0,-1)\n",
    "                        temp_contours = pred_contours.copy()\n",
    "                        del temp_contours[c]\n",
    "                pred_contours = temp_contours.copy()\n",
    "                #create new img without the small contours\n",
    "                img = cv2.bitwise_and(np.uint8(np.rint(test_prediction[k,:,:,1])), np.uint8(np.rint(test_prediction[k,:,:,1])), mask=mask)    \n",
    "                #Last, we need to update the prediction to include only the one contour\n",
    "                test_prediction[k,:,:,1] = img*test_prediction[k,:,:,1]\n",
    "                \n",
    "            #print(\"Length of contour array (should be 1):\", len(pred_contours))\n",
    "            #print(\"Contour length kept (should match largest):\", cv2.contourArea(pred_contours[0]))\n",
    "    \n",
    "            #now we can find the centre of prediction and segmentation contours and compare distances\n",
    "            M = cv2.moments(pred_contours[0])\n",
    "            \n",
    "            if M[\"m00\"]!=0:\n",
    "                pred_cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                pred_cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "                #find the segmentation contour. We segmented so know theres only 1\n",
    "                M = cv2.moments(seg_contours[0])\n",
    "                seg_cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                seg_cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "    \n",
    "                #print(([pred_cX,pred_cY], [seg_cX, seg_cY]))\n",
    "                #now we have pred [x,y] and seg [x,y]. Calculate the distance difference\n",
    "                distance = math.sqrt((pred_cX - seg_cX)**2 + (pred_cY - seg_cY)**2)\n",
    "                count+=1\n",
    "                totDistanceErr += distance\n",
    "    \n",
    "    \n",
    "            '''\n",
    "            Part 3 - overlap calculation. Done inside the if statement so we only calculate if there is a segmentation\n",
    "            '''\n",
    "        \n",
    "            #add margin to the prediction.\n",
    "            accept_pred_map = evaluation_metrics.dilate_stack(test_prediction[k,:,:,1], acceptable_margin_pixel)\n",
    "            accept_pos_map = test_segmentation_data[k,:,:,0]\n",
    "            \n",
    "            #sum together and take locations that are 2 (pred and seg are binary already)\n",
    "            intersection_map = accept_pred_map + accept_pos_map\n",
    "            #this returns a binary array (128x128) where 1 is the intersection, 0 is not\n",
    "            intersection = (intersection_map==2)\n",
    "            \n",
    "            #Calculate area of intersection\n",
    "            area_of_intersection = np.sum(intersection, dtype=\"float64\")\n",
    "            \n",
    "            #Calculate area of segmentation\n",
    "            area_of_segmentation = np.sum(test_segmentation_data[k,:,:,0], dtype=\"float64\")\n",
    "            \n",
    "            #to avoid cases where there are no segmentations\n",
    "            if area_of_segmentation==0: area_of_segmentation=1\n",
    "                \n",
    "            segRate = area_of_intersection/area_of_segmentation\n",
    "            missedRate = 1-segRate\n",
    "            \n",
    "    \n",
    "            #if there is a prediction, store the values\n",
    "            seg.append(segRate)\n",
    "            missed.append(missedRate)\n",
    "            \n",
    "    \n",
    "        '''\n",
    "        Part 4 - Calculate image-wide classification metrics\n",
    "        '''\n",
    "        #Now we will calculate the image-wide metrics\n",
    "        if np.sum(test_segmentation_data[k,:,:,0]) == 0 and np.sum(test_prediction[k,:,:,1]) != 0:\n",
    "            #print(\"predicted tumor but none \",i)\n",
    "            img_FP +=1\n",
    "        elif np.sum(test_segmentation_data[k,:,:,0]) !=0 and np.sum(test_prediction[k,:,:,1]) == 0:\n",
    "            #print(\"predicted no tumor but tumor \",i)\n",
    "            img_FN +=1\n",
    "        elif np.sum(test_segmentation_data[k,:,:,0]) == 0 and np.sum(test_prediction[k,:,:,1]) == 0:\n",
    "            #print(\"predicted no tumor and no tumor \",i)\n",
    "            img_TN +=1\n",
    "        elif np.sum(test_segmentation_data[k,:,:,0]) != 0 and np.sum(test_prediction[k,:,:,1]) != 0:\n",
    "            #print(\"predicted some tumor and tumor \",i)\n",
    "            img_TP +=1\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    Part 5 - Store all metrics for model i\n",
    "    '''\n",
    "    if count == 0: count=1\n",
    "    totDistanceErrs.append(totDistanceErr/count)\n",
    "    \n",
    "    if len(missed)==0: missed=[0]\n",
    "    if len(seg)==0: seg=[0]\n",
    "    totMissed.append(sum(missed)/len(missed))\n",
    "    totSeg.append(sum(seg)/len(seg))\n",
    "    img_TPs.append(img_TP)\n",
    "    img_FPs.append(img_FP)\n",
    "    img_FNs.append(img_FN)\n",
    "    img_TNs.append(img_TN)\n",
    "    img_ACCs.append((img_TP+img_TN)/(img_TP+img_TN+img_FN+img_FP))\n",
    "    \n",
    "    test_prediction_filename = train_timestamp + \"_prediction_test.npy\"\n",
    "    test_prediction_fullname = os.path.join(test_predictions_fullpath, test_prediction_filename)\n",
    "    np.save(test_prediction_fullname, test_prediction)\n",
    "    \n",
    "    '''\n",
    "    Part 6 - Calculate any other metrics in the evaluation_metrics code\n",
    "    '''\n",
    "    # Test results\n",
    "    \n",
    "    test_metrics_dicts, test_best_threshold_index, test_area = evaluation_metrics.compute_roc(\n",
    "        roc_thresholds, test_prediction, test_segmentation_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    test_fuzzy_metrics[i] = evaluation_metrics.compute_evaluation_metrics(\n",
    "        test_prediction, test_segmentation_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    test_best_metrics[i]    = test_metrics_dicts[test_best_threshold_index]\n",
    "    test_aurocs[i]          = test_area\n",
    "    test_best_thresholds[i] = roc_thresholds[test_best_threshold_index]\n",
    "    \n",
    "    print(\"Testing round time: {}\".format(datetime.datetime.now() - time_round_start))\n",
    "\n",
    "\n",
    "time_sequence_stop = datetime.datetime.now()\n",
    "\n",
    "print(\"\\nTotal testing time:   {}\".format(time_sequence_stop - time_sequence_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange results in tables\n",
    "\n",
    "#metric_labels = [\n",
    "#    \"Acc\",\n",
    "#    \"AUROC\",\n",
    "#    \"best thresh\",\n",
    "#    \"best TP\",\n",
    "#    \"best FP\",\n",
    "#    \"best recall\",\n",
    "#    \"best precis\",\n",
    "#    \"best dice\",\n",
    "#    \"fuzzy recall\",\n",
    "#    \"fuzzy precis\",\n",
    "#    \"fuzzy Fscore\"\n",
    "#]\n",
    "\n",
    "metric_labels = [\n",
    "    \"Overall Acc\",\n",
    "    \"Image Classification Acc\",\n",
    "    \"Img Classification TP\",\n",
    "    \"Img Classification FP\",\n",
    "    \"Img Classification FN\",\n",
    "    \"Img Classification TN\",\n",
    "    \"Img Classification Sensitivity (TP/(TP+FN))\",\n",
    "    \"Img Classification Specificity (TN/(TN+FP))\",\n",
    "    \"Img Classification Precision (TP/(TP+FP))\",\n",
    "    \"Avg Center Distance\",\n",
    "    \"Avg Prediction Overlap\",\n",
    "    \"Avg Missed Prediction\",\n",
    "    \"Dice\"    \n",
    "]\n",
    "results_labels = []\n",
    "\n",
    "for label in metric_labels:\n",
    "    results_labels.append(\"Test \" + label)\n",
    "\n",
    "    \n",
    "results_df = pd.DataFrame(columns = results_labels)\n",
    "\n",
    "for i in range(num_models):\n",
    "    results_df.loc[i] = [\n",
    "        test_best_metrics[i][evaluation_metrics.ACCURACY]*100,\n",
    "        img_ACCs[i]*100,\n",
    "        img_TPs[i],\n",
    "        img_FPs[i],\n",
    "        img_FNs[i],\n",
    "        img_TNs[i],\n",
    "        img_TPs[i]/(max(1,img_TPs[i]+img_FNs[i]))*100,\n",
    "        img_TNs[i]/(max(1,img_TNs[i]+img_FPs[i]))*100,\n",
    "        img_TPs[i]/(max(1,img_TPs[i]+img_FPs[i]))*100,\n",
    "        totDistanceErrs[i],\n",
    "        totSeg[i]*100,\n",
    "        totMissed[i]*100,\n",
    "        #test_aurocs[i],\n",
    "        #test_best_thresholds[i],\n",
    "        #test_best_metrics[i][evaluation_metrics.TRUE_POSITIVE_RATE],\n",
    "        #test_best_metrics[i][evaluation_metrics.FALSE_POSITIVE_RATE],\n",
    "        #test_best_metrics[i][evaluation_metrics.RECALL],\n",
    "        #test_best_metrics[i][evaluation_metrics.PRECISION],\n",
    "        test_best_metrics[i][evaluation_metrics.DICE]*100,\n",
    "        #test_fuzzy_metrics[i][evaluation_metrics.RECALL],\n",
    "        #test_fuzzy_metrics[i][evaluation_metrics.PRECISION],\n",
    "        #test_fuzzy_metrics[i][evaluation_metrics.FSCORE],\n",
    "    ]\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\nAverages\")\n",
    "\n",
    "results_means_df = results_df.mean()\n",
    "results_std_df = results_df.std()\n",
    "display(results_means_df)\n",
    "\n",
    "print(\"\\STD\")\n",
    "display(results_std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results table\n",
    "\n",
    "csv_filename = this_notebook_name + \"_\" + train_timestamp + \".csv\"\n",
    "csv_fullname = os.path.join(results_save_fullpath, csv_filename)\n",
    "results_df.to_csv(csv_fullname)\n",
    "\n",
    "print(\"Results saved to: {}\".format(csv_fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "\n",
    "num_test = test_ultrasound_data.shape[0]\n",
    "num_show = 15\n",
    "\n",
    "indices = [i for i in range(num_test)]\n",
    "sample_indices = sample(indices, num_show)\n",
    "\n",
    "fig = plt.figure(figsize=(18, num_show*5))\n",
    "for i in range(num_show):\n",
    "    a0 = fig.add_subplot(num_show,3,i*3+1)\n",
    "    img0 = a0.imshow(test_ultrasound_data[sample_indices[i], :, :, 0].astype(np.float32))\n",
    "    a0.set_title(\"Ultrasound #{}\".format(sample_indices[i]))\n",
    "    a1 = fig.add_subplot(num_show,3,i*3+2)\n",
    "    img1 = a1.imshow(test_segmentation_data[sample_indices[i], :, :, 0], vmin=0.0, vmax=1.0)\n",
    "    a1.set_title(\"Segmentation #{}\".format(sample_indices[i]))\n",
    "    c = fig.colorbar(img1, fraction=0.046, pad=0.04)\n",
    "    a2 = fig.add_subplot(num_show,3,i*3+3)\n",
    "    img2 = a2.imshow(test_prediction[sample_indices[i], :, :, 1], vmin=0.0, vmax=1.0)\n",
    "    a2.set_title(\"Prediction #{}\".format(sample_indices[i]))\n",
    "    c = fig.colorbar(img2, fraction=0.046, pad=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save notebook so all output is archived by the next cell\n",
    "\n",
    "from IPython.display import Javascript\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export HTML copy of this notebook\n",
    "\n",
    "notebook_file_name = this_notebook_name + \"_\" + train_timestamp + \".html\"\n",
    "notebook_fullname = os.path.join(notebooks_save_fullpath, notebook_file_name)\n",
    "\n",
    "os.system(\"jupyter nbconvert --to html \" + this_notebook_name + \" --output \" + notebook_fullname)\n",
    "print(\"Notebook saved to: {}\".format(notebook_fullname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
